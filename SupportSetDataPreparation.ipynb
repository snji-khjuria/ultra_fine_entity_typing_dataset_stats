{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Algorithm:\n",
    "#pick all the classes in clubbed and unclubbed manner and add count threshold to remove the classes with numbers\n",
    "#keep threshold as 5 for clubbed and now put them in unclubbed dictionary and now filter unclubbed ones\n",
    "#check the number of points in that manner\n",
    "#ask for file location and create and put data there.\n",
    "#random sampling algorithm to proceed with the dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the types in dictionary\n",
    "types_location = \"./types.txt\"\n",
    "type2id = {}\n",
    "id2type = {}\n",
    "with open(types_location) as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip().lower()\n",
    "        type2id[line] = i\n",
    "        id2type[i]    = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_locations     = [\"./train\"]\n",
    "complete_data_locations = [\"./train\", \"./dev\", \"./test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def listdir(d):\n",
    "    if not os.path.isdir(d):\n",
    "        return [d]\n",
    "    else:\n",
    "        result=[]\n",
    "        for item in os.listdir(d):\n",
    "            result.extend(listdir((d+'/'+item)))\n",
    "        return result\n",
    "    \n",
    "def get_files(list_of_dirs):\n",
    "    result = []\n",
    "    for d in list_of_dirs:\n",
    "        result.extend(listdir(d))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import operator\n",
    "\n",
    "\n",
    "# def read_data(files):\n",
    "#     types_count = {}\n",
    "#     for f in files:\n",
    "#         for line in open(f, 'r'):\n",
    "#             line = json.loads(line)\n",
    "#             ystr = line['y_str']\n",
    "#             for y in ystr:\n",
    "#                 if not y in types_count:\n",
    "#                     types_count[y] = 0\n",
    "#                 types_count[y] = types_count[y]+1\n",
    "#     return types_count\n",
    "\n",
    "def type_list_to_id(ll):\n",
    "    global type2id\n",
    "    id_l = [str(type2id[l]) for l in ll]\n",
    "    id_l.sort()\n",
    "    return \" \".join(id_l)\n",
    "\n",
    "def id_str_to_type(id_str):\n",
    "    global id2type\n",
    "    ids = id_str.split(\" \")\n",
    "    return \"/\".join([id2type[int(id)] for id in ids])\n",
    "\n",
    "def get_type_id(ll):\n",
    "    ll.sort()\n",
    "    return \" \".join(ll)\n",
    "\n",
    "#consider each class independent\n",
    "def get_types_stats(files, type=\"clubbed\"):\n",
    "    types_count = {}\n",
    "    for json_file in tqdm(files):\n",
    "        for json_point in open(json_file, 'r'):\n",
    "            data_point = json.loads(json_point)\n",
    "            ystr = data_point['y_str']\n",
    "            ystr.sort()\n",
    "            k = \"/\".join(ystr)\n",
    "            k = [k]\n",
    "            if not type==\"clubbed\":\n",
    "                k = ystr\n",
    "            for key in k:\n",
    "                if not key in types_count:\n",
    "                    types_count[key] = 0\n",
    "                types_count[key] = types_count[key]+1\n",
    "    return types_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#write the dictionary for analysis\n",
    "def write_dict(dictionary, output):\n",
    "    sorted_d  = sorted(dictionary.items(), key=operator.itemgetter(0))\n",
    "    types_str = [k+\"\\t\"+str(v) for (k, v) in sorted_d]\n",
    "    types_str = \"\\n\".join(types_str)\n",
    "    with open(output, \"w\") as f:\n",
    "        f.write(types_str)\n",
    "    print(\"written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_locations     = [\"./train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [11:11<00:00, 67.49s/it]\n"
     ]
    }
   ],
   "source": [
    "train_files         = get_files(train_data_locations)\n",
    "clubbed_types_count = get_types_stats(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71411\n",
      "Total number of keys deleted...30687\n",
      "Total number of remaining keys...40724\n"
     ]
    }
   ],
   "source": [
    "#do filtering of the content of clubbed dictionary\n",
    "print(len(clubbed_types_count))\n",
    "del_count=0\n",
    "for k, v in clubbed_types_count.items():\n",
    "    if v<10:\n",
    "        del clubbed_types_count[k]\n",
    "        del_count+=1\n",
    "print(\"Total number of keys deleted...\" + str(del_count))\n",
    "print(\"Total number of remaining keys...\" + str(len(clubbed_types_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the concrete type of specific type\n",
    "def get_concrete_type(t):\n",
    "    loc = t.find(\"/\")\n",
    "    if loc==-1:\n",
    "        return t\n",
    "    else:\n",
    "        return t[:loc]\n",
    "\n",
    "    \n",
    "def create_class_buckets(types_count, threshold=10):\n",
    "    global type2id\n",
    "    types_bucket = {}\n",
    "    for k, v in types_count.items():\n",
    "        concrete_type = get_concrete_type(k)\n",
    "        if not concrete_type in types_bucket:\n",
    "            types_bucket[concrete_type] = 0\n",
    "        types_bucket[concrete_type]+=1\n",
    "    for k, v in types_bucket.items():\n",
    "        if v<threshold:\n",
    "            del types_bucket[k]\n",
    "    return types_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put values in actual dictionary and see the len of dictionaries and now you have list of different kinds of conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_buckets = create_class_buckets(clubbed_types_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of class buckets are \n",
      "948\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of class buckets are \")\n",
    "print(len(class_buckets))\n",
    "sorted_class_buckets = sorted(class_buckets.items(), key=operator.itemgetter(1))\n",
    "sorted_class_buckets = sorted_class_buckets[:200]\n",
    "final_class_buckets = set([a for a, b in sorted_class_buckets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_general_to_specific_class_mapping(general_keys, specific_dict):\n",
    "    output_mapping = {}\n",
    "    classes_of_interest = []\n",
    "    for k, v in specific_dict.items():\n",
    "        concrete_type = get_concrete_type(k)\n",
    "        if concrete_type in general_keys:\n",
    "            if not concrete_type in output_mapping:\n",
    "                output_mapping[concrete_type] = set([])\n",
    "            output_mapping[concrete_type].add(k)\n",
    "            classes_of_interest.append(k)\n",
    "    return output_mapping, set(classes_of_interest)\n",
    "gen_to_specific_type_dict, classes_of_interest = get_general_to_specific_class_mapping(final_class_buckets, clubbed_types_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_locations     = [\"./train\"]\n",
    "train_files         = get_files(train_data_locations)\n",
    "def get_class_specific_training_data(classes_of_interest, files):\n",
    "    class_specific_data = {}\n",
    "    for json_file in tqdm(files):\n",
    "        for json_point in open(json_file, 'r'):\n",
    "            data_point = json.loads(json_point)\n",
    "            ystr = data_point['y_str']\n",
    "            ystr.sort()\n",
    "            k = \"/\".join(ystr)\n",
    "            if k in classes_of_interest:\n",
    "                if not k in class_specific_data:\n",
    "                    class_specific_data[k] = []\n",
    "                class_specific_data[k].append(json_point)\n",
    "    return class_specific_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [11:00<00:00, 61.52s/it]\n"
     ]
    }
   ],
   "source": [
    "class_specific_data = get_class_specific_training_data(classes_of_interest, train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of data points\n",
      "1269423\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k, v in class_specific_data.items():\n",
    "    total+=len(v)\n",
    "    \n",
    "print(\"total number of data points\")\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def get_file_name(k):\n",
    "    return re.sub(\"/\", \"_\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# for k, v in tqdm(class_specific_data.items()):\n",
    "#     concrete_type = get_concrete_type(k)\n",
    "#     directory = \"./support_sets\" + concrete_type\n",
    "#     try:\n",
    "#         shutil.rmtree(directory)\n",
    "#     except OSError as e:\n",
    "#         print(directory)\n",
    "# print(\"written everythng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2166/2166 [00:07<00:00, 282.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written everythng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tqdm(class_specific_data.items()):\n",
    "    concrete_type = get_concrete_type(k)\n",
    "    directory = os.path.join(\"./support_sets\", concrete_type)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    v = \"\".join(v)\n",
    "    fname = os.path.join(directory, get_file_name(k))\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(v)\n",
    "\n",
    "print(\"written everythng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listdir(d):\n",
    "    if not os.path.isdir(d):\n",
    "        return [d]\n",
    "    else:\n",
    "        result=[]\n",
    "        for item in os.listdir(d):\n",
    "            result.extend(listdir((d+'/'+item)))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_support_sets(dir_location):\n",
    "    files = os.listdir(dir_location)\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['broadcasting', 'foot', 'mail', 'hunting', 'shot', 'father', 'campus', 'jet', 'cycling', 'concept', 'strike', 'drought', 'screen', 'daughter', 'broker', 'boxing', 'donor', 'chemistry', 'browser', 'friend', 'support', 'evaluation', 'delegation', 'statement', 'output', 'clan', 'jury', 'adjustment', 'side', 'status', 'engagement', 'lab', 'drinking', 'letter', 'geography', 'forest', 'round', 'burning', 'signal', 'disclosure', 'century', 'farming', 'acid', 'hormone', 'chess', 'cigarette', 'client', 'coup', 'flag', 'cocaine', 'grade', 'platform', 'comeback', 'painting', 'bottle', 'headline', 'bonds', 'copyright', 'commuter', 'ring', 'resource', 'affiliate', 'extradition', 'farmer', 'rebel', 'rain', 'incident', 'fitness', 'commerce', 'arbitration', 'initiative', 'hurricane', 'navy', 'lawmaker', 'entry', 'ambassador', 'batsman', 'basis', 'concession', 'impact', 'bomber', 'opera', 'creation', 'buyer', 'agriculture', 'pool', 'glass', 'discipline', 'item', 'alarm', 'evacuation', 'pact', 'parish', 'pop', 'jail', 'heritage', 'parliament', 'back', 'outlook', 'personnel', 'dairy', 'flash', 'heating', 'jazz', 'stock', 'expectation', 'achievement', 'cleanup', 'census', 'bloc', 'rocket', 'mineral', 'playoff', 'grid', 'hip', 'extension', 'staff', 'broadband', 'organisation', 'mile', 'architecture', 'fall', 'breakthrough', 'bath', 'aerospace', 'license', 'supply', 'marathon', 'default', 'fruit', 'epic', 'method', 'attention', 'borough', 'boy', 'advisor', 'novel', 'reform', 'song', 'component', 'error', 'privacy', 'patrol', 'hybrid', 'counterterrorism', 'athletics', 'summer', 'legislation', 'incentive', 'category', 'billionaire', 'breathing', 'resort', 'consolidation', 'ballot', 'port', 'memorial', 'bass', 'photographer', 'interrogation', 'milk', 'genocide', 'hardline', 'reality', 'chart', 'computing', 'connection', 'dress', 'earth', 'pattern', 'president', 'automobile', 'hero', 'birthday', 'expense', 'muscle', 'regime', 'cricketer', 'boost', 'set', 'doping', 'frame', 'hole', 'documentary', 'deadline', 'review', 'machine', 'sign', 'monarch', 'ownership', 'embassy', 'partner', 'improvement', 'bullet', 'guerrilla', 'close', 'acquisition', 'face', 'stage', 'booth']\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir(\"./support_sets\")\n",
    "support_sets = []\n",
    "for directory in directories:\n",
    "    full_dir_path = os.path.join(\"./support_sets\", directory)\n",
    "    support_sets_per_directory = make_support_sets(full_dir_path)\n",
    "    support_sets.extend(support_sets_per_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_support_set(dir_location, total_support_sets=3, n_way=5, k_shot=1):\n",
    "    data = []\n",
    "    #read the content\n",
    "    for f in os.listdir(dir_location):\n",
    "        file_location = os.path.join(dir_location, f)\n",
    "        with open(file_location, \"r\") as content:\n",
    "            data.append(content.readlines())\n",
    "    output = []\n",
    "    total_classes = len(data)\n",
    "    for _ in range(total_support_sets):\n",
    "        ss = []\n",
    "        x_hat = \"null\"\n",
    "        #create one support set of specific out of these conflicts\n",
    "        classes_to_pick = np.random.permutation(total_classes)[:n_way]\n",
    "        test_point = np.random.permutation(len(classes_to_pick))[0]\n",
    "        for i, c in enumerate(classes_to_pick):\n",
    "            c_data = data[c]\n",
    "            num_samples = len(c_data)\n",
    "#             print(num_samples)\n",
    "            samples = np.random.permutation(num_samples)[:k_shot+1]\n",
    "#             print(\"samples are \")\n",
    "#             print(samples)\n",
    "            for s in samples[:-1]:\n",
    "                ss.append(c_data[s])\n",
    "            if i==test_point:\n",
    "                x_hat = c_data[samples[-1]]\n",
    "        support_set = \"\\n\".join(ss)\n",
    "        support_set+=\"\\n---------------------------------------------------\\n\"\n",
    "        support_set+=x_hat+\"\\n\"\n",
    "        support_set+=\"------------------------------------------------------\\n\\n\"\n",
    "        output.append(support_set)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 239.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the support sets are available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir(\"./support_sets\")\n",
    "support_sets = []\n",
    "for directory in tqdm(directories):\n",
    "    full_dir_path = os.path.join(\"./support_sets\", directory)\n",
    "    support_sets_per_directory = make_support_set(full_dir_path)\n",
    "    support_sets.extend(support_sets_per_directory)\n",
    "support_sets = \"\".join(support_sets)\n",
    "with open(\"./all_support_sets.json\", \"w\") as f:\n",
    "    f.write(support_sets)\n",
    "print(\"All the support sets are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to perform this operation\n",
    "def get_stats_data_frame(types_count):\n",
    "    global type2id\n",
    "    types_bucket = {k:[] for k  in type2id.keys()}\n",
    "    min_bucket   = {k:0  for k  in type2id.keys()}\n",
    "    max_bucket   = {k:0  for k  in type2id.keys()}\n",
    "    total_bucket = {k:0  for k  in type2id.keys()}\n",
    "    for k, v in types_count.items():\n",
    "#         print(k)\n",
    "        concrete_type = get_concrete_type(k)\n",
    "        if not concrete_type in types_bucket:\n",
    "            print(\"issue with the type\")\n",
    "            print(concrete_type)\n",
    "            print(k)\n",
    "            types_bucket[concrete_type]=[]\n",
    "            min_bucket[concrete_type]=0\n",
    "            max_bucket[concrete_type]=0\n",
    "            total_bucket[concrete_type]=0\n",
    "        l = types_bucket[concrete_type]\n",
    "        l.append(k)\n",
    "        types_bucket[concrete_type] = l \n",
    "        min_bucket[concrete_type]   = min(v, min_bucket[concrete_type]) if not min_bucket[concrete_type]==0 else v\n",
    "        max_bucket[concrete_type]   = max(v, max_bucket[concrete_type])\n",
    "        total_bucket[concrete_type] = total_bucket[concrete_type] + v\n",
    "    pollution_per_class = {k:len(v) for k, v in types_bucket.items() if len(v)>0}\n",
    "    types_bucket = {k:\" && \".join(v) for k, v in types_bucket.items() if len(v)>0}\n",
    "    min_bucket  =  {k:v for k, v in min_bucket.items() if k in types_bucket.keys()}\n",
    "    max_bucket  =  {k:v for k, v in max_bucket.items() if k in types_bucket.keys()}\n",
    "    total_bucket  =  {k:v for k, v in total_bucket.items() if k in types_bucket.keys()}\n",
    "    print(\"processed everything\")\n",
    "    return types_bucket, pollution_per_class, min_bucket, max_bucket, total_bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchnet_env",
   "language": "python",
   "name": "matchnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
